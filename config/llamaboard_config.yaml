top.booster: auto
top.checkpoint_path: []
top.finetuning_type: lora
top.model_name: DeepSeek-R1-1.5B-Distill
top.quantization_bit: none
top.quantization_method: bitsandbytes
top.rope_scaling: linear
top.template: deepseek3
train.additional_target: ''
train.apollo_rank: 16
train.apollo_scale: 32
train.apollo_target: all
train.apollo_update_interval: 200
train.badam_mode: layer
train.badam_switch_interval: 50
train.badam_switch_mode: ascending
train.badam_update_ratio: 0.05
train.batch_size: 8
train.compute_type: fp16
train.create_new_adapter: false
train.cutoff_len: 2048
train.dataset:
- mix_new
train.dataset_dir: data
train.ds_offload: false
train.ds_stage: none
train.extra_args: '{"optim": "adamw_torch_fused", "weight_decay": 0.1}'
train.freeze_extra_modules: ''
train.freeze_trainable_layers: 0
train.freeze_trainable_modules: ''
train.galore_rank: 16
train.galore_scale: 2
train.galore_target: all
train.galore_update_interval: 200
train.gradient_accumulation_steps: 2
train.learning_rate: 5e-5
train.logging_steps: 5
train.lora_alpha: 64
train.lora_dropout: 0.1
train.lora_rank: 32
train.lora_target: q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj
train.loraplus_lr_ratio: 8
train.lr_scheduler_type: cosine_with_restarts
train.mask_history: false
train.max_grad_norm: '0.5'
train.max_samples: '100000'
train.neat_packing: false
train.neftune_alpha: 8
train.num_train_epochs: '7'
train.packing: false
train.ppo_score_norm: false
train.ppo_whiten_rewards: false
train.pref_beta: 0.1
train.pref_ftx: 0
train.pref_loss: sigmoid
train.report_to:
- tensorboard
train.resize_vocab: false
train.reward_model: null
train.save_steps: 100
train.swanlab_api_key: ''
train.swanlab_link: ''
train.swanlab_mode: cloud
train.swanlab_project: llamafactory
train.swanlab_run_name: ''
train.swanlab_workspace: ''
train.train_on_prompt: false
train.training_stage: Supervised Fine-Tuning
train.use_apollo: false
train.use_badam: false
train.use_dora: true
train.use_galore: false
train.use_llama_pro: false
train.use_pissa: false
train.use_rslora: true
train.use_swanlab: false
train.val_size: 0.2
train.warmup_steps: 500
