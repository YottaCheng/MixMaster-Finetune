import os
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score, classification_report
import torch

# è·¯å¾„é…ç½®
MODEL_PATH = "/Volumes/Study/prj/models/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa"
DATA_PATH = "/Volumes/Study/prj/data/processed/train_data_en.csv"
MODEL_SAVE_DIR = "/Volumes/Study/prj/data/outputs"
os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

def compute_metrics(pred):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average="macro")
    report = classification_report(labels, preds, output_dict=True)
    return {
        "accuracy": acc,
        "f1": f1,
        "classification_report": report
    }

def main():
    # 1. åŠ è½½æ•°æ®
    df = pd.read_csv(DATA_PATH)
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(df)} æ¡æ ·æœ¬")
    print("æ ‡ç­¾åˆ†å¸ƒ:\n", df["label_en"].value_counts())

    # 2. æ•°æ®é¢„å¤„ç†
    X = df["text_en"].fillna("").str.lower()  # è½¬å°å†™ç»Ÿä¸€å¤„ç†
    y = df["label_en"]

    # å°†æ ‡ç­¾è½¬æ¢ä¸ºæ•°å€¼
    label_map = {label: idx for idx, label in enumerate(y.unique())}
    y = y.map(label_map)

    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    print(f"\næ•°æ®é›†åˆ’åˆ†ï¼šè®­ç»ƒé›† {len(X_train)} æ¡ | æµ‹è¯•é›† {len(X_test)} æ¡")

    # è½¬æ¢ä¸º Hugging Face Dataset æ ¼å¼
    train_dataset = Dataset.from_pandas(pd.DataFrame({"text": X_train, "label": y_train}))
    test_dataset = Dataset.from_pandas(pd.DataFrame({"text": X_test, "label": y_test}))

    # 3. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    
    # ç¡®ä¿åˆ†è¯å™¨æœ‰ pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  # ä½¿ç”¨ eos_token ä½œä¸º pad_token
    
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_PATH, num_labels=len(label_map)
    )

    # ç¡®ä¿æ¨¡å‹çš„åµŒå…¥å±‚åŒ…å« pad_token å¯¹åº”çš„åµŒå…¥å‘é‡
    if model.config.pad_token_id is None:
        model.config.pad_token_id = tokenizer.pad_token_id
        model.resize_token_embeddings(len(tokenizer))  # è°ƒæ•´åµŒå…¥å±‚å¤§å°

    # 4. æ•°æ®é¢„å¤„ç†ï¼šTokenization
    def preprocess_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=64,  # å‡å°‘æœ€å¤§é•¿åº¦ä»¥èŠ‚çœå†…å­˜
            return_tensors="pt"
        )

    train_dataset = train_dataset.map(preprocess_function, batched=True)
    test_dataset = test_dataset.map(preprocess_function, batched=True)

    # è®¾ç½®æ ¼å¼ä¸º PyTorch å¼ é‡
    train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
    test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

    # 5. å¾®è°ƒæ¨¡å‹
    training_args = TrainingArguments(
        output_dir=MODEL_SAVE_DIR,
        eval_strategy="epoch",  # ä½¿ç”¨ eval_strategy æ›¿ä»£ evaluation_strategy
        save_strategy="epoch",
        learning_rate=2e-5,
        per_device_train_batch_size=1,  # è¿›ä¸€æ­¥å‡å°‘æ‰¹é‡å¤§å°
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=8,  # ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿæ›´å¤§æ‰¹é‡
        num_train_epochs=3,
        weight_decay=0.01,
        logging_dir=os.path.join(MODEL_SAVE_DIR, "logs"),
        logging_steps=10,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        # fp16=True,  # ç¦ç”¨ FP16ï¼Œå› ä¸º Mac ä¸æ”¯æŒ
        dataloader_num_workers=0,  # ä½¿ç”¨å•çº¿ç¨‹æ•°æ®åŠ è½½å™¨
        remove_unused_columns=False,  # ä¿ç•™æ‰€æœ‰åˆ—
        report_to="none",  # ç¦ç”¨æ—¥å¿—æŠ¥å‘Š
        device="cpu",  # å¼ºåˆ¶ä½¿ç”¨ CPU
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    # å¼€å§‹è®­ç»ƒ
    trainer.train()

    # 6. è¯„ä¼°æ¨¡å‹
    eval_results = trainer.evaluate()
    print("\n=== æµ‹è¯•é›†æ€§èƒ½ ===")
    print("å‡†ç¡®ç‡ï¼š", eval_results["eval_accuracy"])
    print("å®å¹³å‡F1ï¼š", eval_results["eval_f1"])
    print("\nåˆ†ç±»æŠ¥å‘Šï¼š\n", eval_results["eval_classification_report"])

    # 7. ä¿å­˜æ¨¡å‹
    trainer.save_model(os.path.join(MODEL_SAVE_DIR, "fine_tuned_model"))
    tokenizer.save_pretrained(os.path.join(MODEL_SAVE_DIR, "fine_tuned_tokenizer"))
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ {MODEL_SAVE_DIR}")

if __name__ == "__main__":
    main()