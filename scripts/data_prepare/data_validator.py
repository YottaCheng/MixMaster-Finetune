import json
import os
import csv
import random
import shutil
from pathlib import Path
from collections import defaultdict
from transformers import AutoTokenizer



MODEL_DIR = "/Volumes/Study/prj/huggingface_cache/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562"

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_DIR,
    use_fast=True,  # ä½¿ç”¨å¿«é€Ÿåˆ†è¯å™¨
    add_special_tokens=False  # é¿å…åŠ¨æ€æ·»åŠ ç‰¹æ®Šæ ‡è®°
)
def validate_data(log_path, json_path, label_mapping_path):
    """ä¸»éªŒè¯æµç¨‹"""
    create_backup(json_path)
    if not validate_files(log_path, json_path, label_mapping_path):
        return False
    log_data, cleaned_data, valid_labels = load_data(log_path, json_path, label_mapping_path)
    if not all([log_data, cleaned_data, valid_labels]):
        return False
    model_valid = validate_model(MODEL_DIR)
    print(f"\nğŸ”§ æ¨¡å‹éªŒè¯ç»“æœ: {'âœ… é€šè¿‡' if model_valid else 'âš ï¸ è­¦å‘Šï¼ˆå¯å°è¯•ç»§ç»­ï¼‰'}")
    check_result = intelligent_sampling_check(cleaned_data, json_path)
    generate_final_report(log_data, cleaned_data, model_valid, check_result)
    return check_result and model_valid

def validate_files(*paths):
    """éªŒè¯æ–‡ä»¶å­˜åœ¨æ€§"""
    missing = [str(p) for p in paths if not Path(p).exists()]
    if missing:
        print(f"âŒ ç¼ºå¤±æ–‡ä»¶: {', '.join(missing)}")
        return False
    return True

def load_data(log_path, json_path, label_mapping_path):
    """åŠ è½½æ‰€æœ‰æ•°æ®"""
    try:
        with open(log_path) as f:
            log_data = json.load(f)
        with open(json_path) as f:
            cleaned_data = json.load(f)
        with open(label_mapping_path, encoding='utf-8') as f:
            valid_labels = {row['ä¸­æ–‡æ ‡ç­¾'].strip() for row in csv.DictReader(f)}
        return log_data, cleaned_data, valid_labels
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        return None, None, None

def validate_model(model_dir):
    """å¢å¼ºå‹æ¨¡å‹éªŒè¯ï¼ˆåŒ…å«åŠŸèƒ½æ€§æµ‹è¯•ï¼‰"""
    essential_files = {'config.json', 'model.safetensors'}  # æ›¿æ¢ pytorch_model.bin ä¸º model.safetensors
    try:
        model_path = Path(model_dir)
        existing_files = {f.name for f in model_path.glob('*')}
        missing = essential_files - existing_files
        
        if missing:
            print(f"âŒ ç¼ºå¤±å…³é”®æ–‡ä»¶: {', '.join(missing)}")
            return False
        
        # åŠŸèƒ½æ€§éªŒè¯
        print("â³ æ­£åœ¨è¿›è¡Œæ¨¡å‹åŠ è½½æµ‹è¯•...", end="")
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                model_dir,
                use_fast=True,  # ä½¿ç”¨å¿«é€Ÿåˆ†è¯å™¨
                add_special_tokens=False  # é¿å…åŠ¨æ€æ·»åŠ ç‰¹æ®Šæ ‡è®°
            )
            model = AutoModelForCausalLM.from_pretrained(model_dir)  # åŠ è½½æ”¯æŒç”Ÿæˆä»»åŠ¡çš„æ¨¡å‹
        except Exception as e:
            print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False
        
        # æ–‡æœ¬ç”Ÿæˆæµ‹è¯•
        print("\nâ³ æ­£åœ¨è¿›è¡Œæ–‡æœ¬ç”Ÿæˆæµ‹è¯•...", end="")
        test_input = "ä½ å¥½ï¼Œæˆ‘æ˜¯Qwen"
        inputs = tokenizer(test_input, return_tensors="pt")
        
        try:
            outputs = model.generate(**inputs, max_new_tokens=10)  # è°ƒç”¨ .generate()
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"\nâœ… ç”Ÿæˆæµ‹è¯•ç»“æœ: {generated_text}")
            return True
        except Exception as e:
            print(f"\nâŒ æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {str(e)}")
            return False

    except Exception as e:
        print(f"âŒ æ¨¡å‹éªŒè¯å¼‚å¸¸: {str(e)}")
        return False
    
def intelligent_sampling_check(data, json_path, sample_size=10):
    """æ™ºèƒ½æŠ½æ ·æ£€æŸ¥ä¸è‡ªåŠ¨æ¸…ç†"""
    print(f"\nğŸ” å¼€å§‹æ™ºèƒ½æŠ½æ ·æ£€æŸ¥ï¼ˆå…±{sample_size}æ¡ï¼‰:")
    
    # ç›´æ¥éšæœºæŠ½æ ·ï¼ˆæ³¨é‡Šå¯ç–‘æ ·æœ¬æ£€æµ‹ï¼‰
    check_samples = random.sample(data, sample_size)
    
    # æ‰§è¡Œæ£€æŸ¥
    invalid_ids = process_samples(check_samples, data)
    
    # è‡ªåŠ¨æ¸…ç†æ•°æ®
    cleanup_data(json_path, data, invalid_ids)
        
    return len(invalid_ids) == 0

def detect_suspicious_samples(data):
    """æ£€æµ‹å¯ç–‘æ ·æœ¬"""
    suspicious = []
    seen_texts = set()
    
    for item in data:
        # ç©ºæ–‡æœ¬æ£€æµ‹
        if len(item['text'].strip()) < 3:
            suspicious.append(item)
            continue
            
        # é‡å¤å†…å®¹æ£€æµ‹
        text = item['text'].strip().lower()
        if text in seen_texts:
            suspicious.append(item)
        else:
            seen_texts.add(text)
    
    return suspicious

def select_samples(data, suspicious, sample_size):
    """é€‰æ‹©æ£€æŸ¥æ ·æœ¬"""
    # ä¼˜å…ˆé€‰æ‹©å¯ç–‘æ ·æœ¬
    check_samples = suspicious[:sample_size//2] if suspicious else []
    
    # è¡¥å……éšæœºæ ·æœ¬
    remaining = sample_size - len(check_samples)
    if remaining > 0:
        check_samples += random.sample(data, remaining)
    
    # å»é‡å¤„ç†
    return list({item['id']: item for item in check_samples}.values())[:sample_size]

def process_samples(samples, full_data):
    """å¤„ç†æ ·æœ¬æ£€æŸ¥"""
    invalid_ids = set()
    
    for idx, item in enumerate(samples, 1):
        print(f"\n[{idx}/{len(samples)}] ID:{item['id']}")
        print(f"æ ‡ç­¾: {item['label']}")
        print(f"å†…å®¹: {item['text'][:150]}...")
        
        # ç§»é™¤è‡ªåŠ¨é‡å¤æ£€æµ‹ï¼ˆå·²æ³¨é‡Šï¼‰
        # if is_duplicate(item, full_data):
        #     print("âš ï¸ æ£€æµ‹åˆ°é‡å¤å†…å®¹ï¼")
        
        # ç®€åŒ–äº¤äº’é€»è¾‘
        choice = input("æ˜¯å¦ä¿ç•™è¯¥æ ·æœ¬ï¼Ÿ(y-ä¿ç•™/n-åˆ é™¤): ").lower()
        if choice == 'n':
            invalid_ids.add(item['id'])
        elif choice != 'y':
            print("è¾“å…¥é”™è¯¯ï¼Œé»˜è®¤ä¿ç•™")
    
    return invalid_ids

def is_duplicate(item, data):
    """æ£€æŸ¥é‡å¤å†…å®¹"""
    return sum(1 for x in data if x['text'].strip().lower() == item['text'].strip().lower()) > 1

def cleanup_data(json_path, data, invalid_ids):
    """æ¸…ç†æ— æ•ˆæ•°æ®"""
    valid_data = [item for item in data if item['id'] not in invalid_ids]
    
    try:
        with open(json_path, 'w') as f:
            json.dump(valid_data, f, indent=2)
        print(f"\nâ™»ï¸ å·²åˆ é™¤ {len(invalid_ids)} æ¡æ— æ•ˆæ•°æ®")
    except Exception as e:
        print(f"âŒ æ•°æ®æ¸…ç†å¤±è´¥: {str(e)}")

def create_backup(file_path):
    """åˆ›å»ºæ•°æ®å¤‡ä»½"""
    backup_path = f"{file_path}.bak"
    if not Path(backup_path).exists():
        try:
            shutil.copy(file_path, backup_path)
        except Exception as e:
            print(f"âš ï¸ å¤‡ä»½å¤±è´¥: {str(e)}")

def generate_final_report(log_data, data, model_valid, check_result):
    """æ”¹è¿›åçš„æœ€ç»ˆæŠ¥å‘Š"""
    print("\nğŸ“ æœ€ç»ˆéªŒè¯æŠ¥å‘Š:")
    print(f"- æ•°æ®æ€»é‡: {len(data)} æ¡ (æ—¥å¿—è®°å½•: {log_data['summary']['total_samples']})")
    print(f"- æ¨¡å‹éªŒè¯: {'âœ… æ­£å¸¸' if model_valid else 'âš ï¸ ç¼ºå¤±å…³é”®æ–‡ä»¶'}")
    print(f"- æ ·æœ¬æ£€æŸ¥: {'âœ… é€šè¿‡' if check_result else 'âš ï¸ éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥'}")
    print(f"- æ¨èæ“ä½œ: {'' if check_result and model_valid else 'è¯·é‡æ–°è¿è¡ŒéªŒè¯'}")

if __name__ == "__main__":
    log_path = "/Volumes/Study/prj/data/processed/data_quality_report.json"
    json_path = "/Volumes/Study/prj/data/processed/cleaned_data.json"
    label_mapping = "/Volumes/Study/prj/data/raw/label_mapping.csv"
    
    validate_data(log_path, json_path, label_mapping)