"""éŸ³ä¹éœ€æ±‚å¢å¼ºå¤„ç†è„šæœ¬ v3.0
(æ”¯æŒäº”ç§æ•°æ®å¢å¼ºæ–¹å¼)

è®ºæ–‡æ ¸å¿ƒå¼•ç”¨ï¼š
1. å—é™é‡‡æ ·ï¼ˆRestricted Samplingï¼‰ï¼š
   "Restricted sampling appears to be the most consistent approach, always scaling with larger monolingual data." (Section 5.3)
2. ç¦ç”¨æ ‡ç­¾å¹³æ»‘ï¼š
   "Disabling label smoothing for the target-to-source model [...] results in higher-quality synthetic data." (Section 4.1)
3. N-beståˆ—è¡¨é‡‡æ ·ï¼š
   "50-best sampling improves significantly in both test sets." (Section 5.3)
"""
"""éŸ³ä¹éœ€æ±‚å¢å¼ºå¤„ç†è„šæœ¬ v3.1ï¼ˆç¨³å®šäº”ç»“æœç‰ˆï¼‰"""
"""éŸ³ä¹éœ€æ±‚å¢å¼ºå¤„ç†è„šæœ¬ v3.2ï¼ˆè®ºæ–‡ä¼˜åŒ–ç‰ˆï¼‰"""

import os
import re
import sys
from pathlib import Path
import time
from datetime import datetime
from docx import Document
from docx.shared import Pt
import dashscope
from dashscope import Generation

# ======================
# åŸºç¡€é…ç½®ï¼ˆè®ºæ–‡å‚æ•°ä¼˜åŒ–ï¼‰
# ======================
BASE_DIR = Path("/Volumes/Study/prj")
CONFIG = {
    "api_key": "sk-3b986ed51abb4ed18aadde5d41e11397",
    "input_path": BASE_DIR / "data/raw/training_raw_data.docx",
    "output_path": BASE_DIR / "data/processed/backtrans_results.docx",
    "log_path": BASE_DIR / "data/processed/processing.log",
    
    # è®ºæ–‡å‚æ•°ä¼˜åŒ–
    "sampling": {
        "base_temp": 0.3,     # åŸºç¡€æ¸©åº¦ï¼ˆå¯¹åº”ç¦ç”¨æ ‡ç­¾å¹³æ»‘ï¼‰
        "high_temp": 0.6,     # åä¹‰ç”Ÿæˆæ¸©åº¦ï¼ˆN-besté‡‡æ ·ï¼‰
        "top_p": 0.85,        # å—é™é‡‡æ ·é˜ˆå€¼
        "max_retry": 3        # æœ€å¤§é‡è¯•æ¬¡æ•°
    },
    
    # æ­£åˆ™è¿‡æ»¤æ¨¡å¼ï¼ˆç½®ä¿¡åº¦æ©ç ï¼‰
    "patterns": {
        "antonym": r"(ç›¸åçš„æè¿°ï¼š|å¯ä»¥è¿™æ ·è¡¨è¿°ï¼š|è¿™æ„å‘³ç€).*",
        "synonym": r"(æ”¹å†™åçš„å¥å­ï¼š|å»ºè®®æ”¹ä¸ºï¼š)",
        "default": r"[\(ï¼ˆ].*?[\)ï¼‰]"
    }
}

class RobustAugmenter:
    def __init__(self):
        # åˆå§‹åŒ–APIè¿æ¥
        dashscope.api_key = CONFIG["api_key"]
        dashscope.base_url = "https://dashscope-intl.aliyuncs.com/compatible-mode/v1"
        
        # æ–‡ä»¶éªŒè¯
        if not CONFIG["input_path"].exists():
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{CONFIG['input_path']}")
        CONFIG["output_path"].parent.mkdir(parents=True, exist_ok=True)
        
        # å¢å¼ºç»Ÿè®¡ç³»ç»Ÿ
        self.stats = {
            "total": 0, "success": 0, "failed": 0,
            "input_tokens": 0, "output_tokens": 0,
            "retries": 0
        }
        self.start_time = time.time()

    def _call_api(self, prompt: str, text: str, is_critical: bool = False) -> str:
        """API calling (with restart)"""
        params = {
            "model": "qwen-max",
            "prompt": f"{prompt}\n{text}",
            "temperature": CONFIG["sampling"]["high_temp"] if is_critical else CONFIG["sampling"]["base_temp"],
            "top_p": CONFIG["sampling"]["top_p"],
            "max_tokens": 100  
        }
        
        for attempt in range(CONFIG["sampling"]["max_retry"]):
            try:
                response = Generation.call(**params)
                
                if not hasattr(response, 'output') or not response.output.text:
                    raise ValueError("æ— æ•ˆAPIå“åº”ç»“æ„")
                
                if hasattr(response, 'usage'):
                    self.stats["input_tokens"] += response.usage.input_tokens
                    self.stats["output_tokens"] += response.usage.output_tokens
                
                return response.output.text.strip()
                
            except Exception as e:
                error_type = type(e).__name__
                print(f"APIå¼‚å¸¸ï¼ˆ{attempt+1}/{CONFIG['sampling']['max_retry']}ï¼‰[{error_type}]: {str(e)}")
                self.stats["retries"] += 1
                time.sleep(2 ** attempt)
        
        return "[ç”Ÿæˆå¤±è´¥]"

    def _clean_output(self, text: str, mode: str) -> str:
        """åå¤„ç†æ¨¡å—ï¼ˆåŸºäºè®ºæ–‡ç½®ä¿¡åº¦è¿‡æ»¤ï¼‰"""
        # æ¨¡å¼åŒ¹é…å†—ä½™å†…å®¹
        patterns = {
            "antonym": CONFIG["patterns"]["antonym"],
            "synonym": CONFIG["patterns"]["synonym"],
            "default": CONFIG["patterns"]["default"]
        }
        
        # åˆ†æ­¥æ¸…æ´—
        clean_text = re.sub(patterns.get(mode, patterns["default"]), "", text)
        clean_text = re.sub(r"\s+", " ", clean_text)  # åˆå¹¶å¤šä½™ç©ºæ ¼
        clean_text = clean_text.strip("â€â€œ\"'ï¼šã€")  # å»é™¤è¾¹ç¼˜ç¬¦å·
        
        # æˆªæ–­å¤„ç†ï¼ˆä¿æŒæ ¸å¿ƒè¯­ä¹‰ï¼‰
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', clean_text)
        return sentences[0].strip() if sentences else clean_text

    def _generate_results(self, text: str) -> tuple:
        """ç”Ÿæˆä¼˜åŒ–åçš„äº”ç»“æœï¼ˆå¸¦è®ºæ–‡æ–¹æ³•çº¦æŸï¼‰"""
        # åå‘ç¿»è¯‘å¢å¼ºï¼ˆSection 3.1ï¼‰
        en_trans = self._clean_output(
            self._call_api("ä¸¥æ ¼ç¿»è¯‘ä¸ºè‹±æ–‡ï¼ˆä»…è¾“å‡ºç»“æœï¼‰ï¼š", text),
            "default"
        )
        back_trans = self._clean_output(
            self._call_api("å°†è‹±æ–‡å›è¯‘ä¸ºä¸­æ–‡ï¼ˆä»…è¾“å‡ºç»“æœï¼‰ï¼š", en_trans),
            "default"
        )
        
        # åŒä¹‰æ›¿æ¢ï¼ˆSection 4.2.1ï¼‰
        synonym_pro = self._clean_output(
            self._call_api("ç”¨ä¸“ä¸šéŸ³ä¹æœ¯è¯­æ”¹å†™ï¼ˆä¿æŒåŸæ„ï¼‰ï¼š", text),
            "synonym"
        )
        synonym_pop = self._clean_output(
            self._call_api("ç”¨é€šä¿—è¯­è¨€ç®€åŒ–è¡¨è¾¾ï¼š", text),
            "synonym"
        )
        
        # åä¹‰ç”Ÿæˆï¼ˆSection 4.2.2ï¼‰
        antonym = self._clean_output(
            self._call_api("ç”Ÿæˆå®Œå…¨ç›¸åçš„éŸ³ä¹éœ€æ±‚ï¼ˆä»…è¾“å‡ºéœ€æ±‚æœ¬èº«ï¼‰ï¼š", text, is_critical=True),
            "antonym"
        )
        
        return (en_trans, back_trans, synonym_pro, synonym_pop, antonym)

    def _save_report(self):
        """å¢å¼ºç‰ˆæŠ¥å‘Šç”Ÿæˆ"""
        elapsed = time.time() - self.start_time
        report = f"""
{' éŸ³ä¹éœ€æ±‚å¢å¼ºæŠ¥å‘Š '.center(40, '=')}
å¤„ç†æ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
========================================
æˆåŠŸå¤„ç†: {self.stats['success']}/{self.stats['total']} 
APIé‡è¯•: {self.stats['retries']}æ¬¡
Tokenç”¨é‡: è¾“å…¥ {self.stats['input_tokens']} | è¾“å‡º {self.stats['output_tokens']}
å¹³å‡è€—æ—¶: {elapsed/(self.stats['success'] or 1):.2f}ç§’/æ¡
æ€»è€—æ—¶: {self._format_duration(elapsed)}
========================================"""
        with open(CONFIG["log_path"], "w") as f:
            f.write(report)
        print(report)

    def _format_duration(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´å·®"""
        hours, rem = divmod(seconds, 3600)
        minutes, seconds = divmod(rem, 60)
        return f"{int(hours):02}:{int(minutes):02}:{int(seconds):02}"

    def process(self):
        """ä¸»å¤„ç†æµç¨‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        print("ğŸµ å¯åŠ¨éŸ³ä¹éœ€æ±‚å¢å¼ºï¼ˆè®ºæ–‡ä¼˜åŒ–ç‰ˆï¼‰...")
        input_doc = Document(CONFIG["input_path"])
        output_doc = Document()
        
        try:
            valid_paras = [p.text.strip() for p in input_doc.paragraphs if p.text.strip()]
            total = len(valid_paras)
            
            for idx, text in enumerate(valid_paras, 1):
                self.stats["total"] += 1
                try:
                    results = self._generate_results(text)
                    
                    # ç»“æ„åŒ–å†™å…¥
                    output_doc.add_paragraph(f"ã€åŸå§‹ã€‘{text}")
                    output_doc.add_paragraph(f"1. è‹±è¯‘ â†’ {results[0]}")
                    output_doc.add_paragraph(f"2. å›è¯‘ â†’ {results[1]}")
                    output_doc.add_paragraph(f"3. ä¸“ä¸š â†’ {results[2]}")
                    output_doc.add_paragraph(f"4. é€šä¿— â†’ {results[3]}")
                    output_doc.add_paragraph(f"5. åä¹‰ â†’ {results[4]}")
                    
                    self.stats["success"] += 1
                    print(f"è¿›åº¦: {idx}/{total} ({idx/total:.0%}) | æœ€æ–°: {results[1][:20]}...", flush=True)
                    
                    # åŠ¨æ€é™æµï¼ˆ1-3ç§’éšæœºå»¶è¿Ÿï¼‰
                    time.sleep(min(max(idx % 3, 1), 3))
                    
                except Exception as e:
                    print(f"å¤„ç†å¤±è´¥: {text[:15]}... | é”™è¯¯: {type(e).__name__}-{str(e)}")
                    self.stats["failed"] += 1
                    
            output_doc.save(CONFIG["output_path"])
            print(f"\nâœ… ç»“æœå·²ä¿å­˜è‡³: {CONFIG['output_path']}")
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼æ­£åœ¨ä¿å­˜å·²å¤„ç†å†…å®¹...")
            output_doc.save(CONFIG["output_path"])
        finally:
            self._save_report()

if __name__ == "__main__":
    try:
        RobustAugmenter().process()
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {type(e).__name__}-{str(e)}")
