"""
æ··éŸ³éœ€æ±‚å¢å¼ºç³»ç»Ÿ - æ¨¡å‹å¢å¼ºç‰ˆ v1.0
"""
import os
import json
import re
import time
import jieba
import torch
from docx import Document
from docx.shared import Pt
from transformers import AutoTokenizer, AutoModelForCausalLM
from concurrent.futures import ThreadPoolExecutor

# é…ç½®è·¯å¾„
MODEL_PATH = "/Volumes/Study/prj/models/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa"
INPUT_FILE = "/Volumes/Study/prj/data/raw/training_raw_data.docx"
OUTPUT_FILE = "/Volumes/Study/prj/data/processed/augmented_model.docx"

# å…¨å±€é…ç½®
MAX_TEXT_LENGTH = 150
MIN_TEXT_LENGTH = 8
BATCH_SIZE = 16

class ModelAugmentor:
    def __init__(self):
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        self.tokenizer, self.model = self._load_model()
        self.executor = ThreadPoolExecutor(max_workers=2)
        jieba.initialize()
        print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def _load_model(self):
        try:
            tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                device_map=self.device,
                torch_dtype=torch.float16,
                trust_remote_code=True
            ).eval()
            print(f"ğŸ›ï¸ æ¨¡å‹åŠ è½½æˆåŠŸ | è®¾å¤‡: {model.device}")
            return tokenizer, model
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")

    def _generate_batch(self, texts):
        prompts = [f"ç”Ÿæˆä¸“ä¸šæ··éŸ³å˜ä½“ï¼š{text}" for text in texts]
        inputs = self.tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=128).to(self.device)
        
        outputs = self.model.generate(
            inputs.input_ids,
            max_new_tokens=64,
            temperature=0.6,
            top_p=0.9,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id
        )
        
        return [
            re.findall(r"å˜ä½“[:ï¼š]?\s*(.+?)(?=\n|$)", 
            self.tokenizer.decode(o, skip_special_tokens=True))[:3] 
            for o in outputs
        ]

    def process_text(self, text):
        variants = self._generate_batch([text])[0]
        return [
            v for v in variants
            if (MIN_TEXT_LENGTH <= len(v) <= MAX_TEXT_LENGTH)
            and not re.search(r"[\[\]ã€ã€‘()ï¼ˆï¼‰]", v)
        ][:3]

def main():
    augmentor = ModelAugmentor()
    doc = Document()
    doc.styles['Normal'].font.name = 'å¾®è½¯é›…é»‘'
    
    input_doc = Document(INPUT_FILE)
    paragraphs = [p.text.strip() for p in input_doc.paragraphs if p.text.strip()]
    
    start_time = time.time()
    for idx, text in enumerate(paragraphs, 1):
        try:
            variants = augmentor.process_text(text)
            doc.add_paragraph(f"åŸå§‹éœ€æ±‚ï¼š{text}").bold = True
            if variants:
                doc.add_paragraph("æ¨¡å‹ç”Ÿæˆï¼š" + " | ".join(variants))
            print(f"\rå¤„ç†è¿›åº¦: {idx}/{len(paragraphs)} | è€—æ—¶: {time.time()-start_time:.1f}s", end="")
        except Exception as e:
            print(f"\nâš ï¸ å¤„ç†å¤±è´¥: {text[:20]}...")
    
    doc.save(OUTPUT_FILE)
    print(f"\nâœ… å¢å¼ºå®Œæˆï¼ä¿å­˜è‡³: {OUTPUT_FILE}")

if __name__ == "__main__":
    print("=== æ¨¡å‹å¢å¼ºç³»ç»Ÿ ===")
    start = time.time()
    try:
        main()
    except KeyboardInterrupt:
        print("\næ“ä½œå·²ä¸­æ–­")
    finally:
        print(f"æ€»è€—æ—¶: {time.time()-start:.1f}ç§’")
        