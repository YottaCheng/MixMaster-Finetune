import streamlit as st
import json
import os
import re
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm.auto import tqdm
import pandas as pd
import numpy as np
import random
import time
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix
import seaborn as sns
from datetime import datetime
import uuid
import base64
import io

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="éŸ³é¢‘å‚æ•°æ¨¡å‹è¯„ä¼°å·¥å…·",
    page_icon="ğŸ§",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSSæ ·å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.8rem;
        color: #333;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .metric-card {
        background-color: #f5f5f5;
        border-radius: 8px;
        padding: 15px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        margin-bottom: 1rem;
    }
    .metric-title {
        font-weight: bold;
        color: #555;
        margin-bottom: 0.3rem;
    }
    .metric-value {
        font-size: 1.5rem;
        font-weight: bold;
        color: #1E88E5;
    }
    .metric-comparison {
        font-size: 0.9rem;
        color: #4CAF50;
    }
    .negative-comparison {
        color: #F44336;
    }
    .example-container {
        background-color: #f9f9f9;
        border-left: 4px solid #2196F3;
        padding: 10px;
        margin: 10px 0;
        border-radius: 0 4px 4px 0;
    }
    .tab-container {
        padding: 20px;
        border: 1px solid #ddd;
        border-radius: 8px;
        margin-top: 10px;
    }
    footer {
        text-align: center;
        margin-top: 2rem;
        padding: 1rem;
        color: #777;
        font-size: 0.9rem;
    }
    .prediction-example {
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        padding: 15px;
        margin-bottom: 15px;
        background-color: white;
    }
    .prediction-text {
        font-size: 1.1rem;
        color: #444;
    }
    .correct-prediction {
        color: #4CAF50;
        font-weight: bold;
    }
    .incorrect-prediction {
        color: #F44336;
        font-weight: bold;
    }
    .custom-progress {
        height: 20px;
        border-radius: 10px;
    }
    .help-text {
        color: #777;
        font-size: 0.9rem;
        font-style: italic;
    }
    .comparison-highlight {
        background-color: #e3f2fd;
        padding: 5px;
        border-radius: 4px;
    }
</style>
""", unsafe_allow_html=True)

# å¸¸é‡å®šä¹‰
VALID_LABELS = {'ä½é¢‘', 'ä¸­é¢‘', 'é«˜é¢‘', 'reverb', 'æ•ˆæœå™¨', 'å£°åœº', 'å‹ç¼©', 'éŸ³é‡'}
DEFAULT_MAX_SAMPLES = 50
DEFAULT_SEED = 42

# å·¥å…·å‡½æ•°
def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡å¤"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def parse_labels(text):
    """è§£æå¹¶æ ‡å‡†åŒ–æ ‡ç­¾"""
    if not text or not isinstance(text, str):
        return []
        
    text = text.strip().lower()
    # å¤„ç†ä¸­è‹±æ–‡æ ‡ç‚¹å’Œç©ºæ ¼
    raw_labels = [label.strip() for label in re.split(r'[,ï¼Œã€]', text) if label.strip()]
    
    # éªŒè¯æ ‡ç­¾å¹¶æ£€æµ‹ä¹±ç 
    validated_labels = []
    for label in raw_labels:
        # æ£€æµ‹ä¹±ç 
        if re.search(r'[\ufffd\uFFFD]', label):
            # å°è¯•ä¿®å¤å¸¸è§ä¹±ç 
            if 'å‹' in label and 'ç¼©' in label:
                validated_labels.append('å‹ç¼©')
            else:
                st.warning(f"æ£€æµ‹åˆ°ä¹±ç æ ‡ç­¾ '{label}'")
        else:
            # è§„èŒƒåŒ–æ ‡ç­¾
            if label in VALID_LABELS:
                validated_labels.append(label)
            # å¤„ç†è¿‘ä¼¼åŒ¹é…
            elif 'ä½' in label and 'é¢‘' in label:
                validated_labels.append('ä½é¢‘')
            elif 'ä¸­' in label and 'é¢‘' in label:
                validated_labels.append('ä¸­é¢‘')
            elif 'é«˜' in label and 'é¢‘' in label:
                validated_labels.append('é«˜é¢‘')
            elif 'reverb' in label or 'æ··å“' in label:
                validated_labels.append('reverb')
            elif 'æ•ˆæœ' in label:
                validated_labels.append('æ•ˆæœå™¨')
            elif 'å£°åœº' in label:
                validated_labels.append('å£°åœº')
            elif 'å‹' in label and 'ç¼©' in label:
                validated_labels.append('å‹ç¼©')
            elif 'éŸ³é‡' in label or 'éŸ³é‡' in label:
                validated_labels.append('éŸ³é‡')
            
    return validated_labels

def extract_prediction(text, prompt):
    """ä»æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–é¢„æµ‹æ ‡ç­¾"""
    # ç§»é™¤promptéƒ¨åˆ†
    if prompt in text:
        text = text[len(prompt):]
    
    # å°è¯•æŸ¥æ‰¾"è¾“å‡ºï¼š"éƒ¨åˆ†
    if "è¾“å‡ºï¼š" in text:
        text = text.split("è¾“å‡ºï¼š")[1].strip()
    
    # æ¸…ç†æ–‡æœ¬
    text = re.sub(r'^[\s:"ï¼š]+', '', text)
    text = re.sub(r'[\n\r]+', ' ', text)
    
    # è§£ææ ‡ç­¾
    return parse_labels(text)

def get_download_link(data, filename, text):
    """ç”Ÿæˆä¸‹è½½é“¾æ¥"""
    if isinstance(data, pd.DataFrame):
        csv = data.to_csv(index=False)
        b64 = base64.b64encode(csv.encode()).decode()
    else:
        b64 = base64.b64encode(json.dumps(data, indent=2, ensure_ascii=False).encode()).decode()
    href = f'<a href="data:file/txt;base64,{b64}" download="{filename}">{text}</a>'
    return href

def generate_html_report(metrics, examples, config):
    """ç”ŸæˆHTMLè¯„ä¼°æŠ¥å‘Š"""
    base_metrics = metrics["base_model"]
    ft_metrics = metrics["finetuned_model"]
    
    # åˆ›å»ºHTMLæŠ¥å‘Š
    report = f"""
    <!DOCTYPE html>
    <html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>éŸ³é¢‘å‚æ•°æ¨¡å‹è¯„ä¼°æŠ¥å‘Š</title>
        <style>
            body {{
                font-family: Arial, sans-serif;
                line-height: 1.6;
                color: #333;
                max-width: 1200px;
                margin: 0 auto;
                padding: 20px;
            }}
            h1, h2, h3 {{
                color: #1E88E5;
            }}
            h1 {{
                text-align: center;
                border-bottom: 2px solid #eee;
                padding-bottom: 10px;
            }}
            .report-meta {{
                background-color: #f5f5f5;
                padding: 15px;
                border-radius: 5px;
                margin-bottom: 20px;
            }}
            table {{
                width: 100%;
                border-collapse: collapse;
                margin: 20px 0;
            }}
            th, td {{
                padding: 12px 15px;
                border: 1px solid #ddd;
                text-align: left;
            }}
            th {{
                background-color: #f8f8f8;
            }}
            tr:nth-child(even) {{
                background-color: #f2f2f2;
            }}
            .metric-table th:first-child {{
                width: 30%;
            }}
            .example-container {{
                background-color: #f9f9f9;
                border-left: 4px solid #2196F3;
                padding: 15px;
                margin: 15px 0;
                border-radius: 0 4px 4px 0;
            }}
            .correct {{
                color: #4CAF50;
                font-weight: bold;
            }}
            .incorrect {{
                color: #F44336;
                font-weight: bold;
            }}
            .improvement {{
                color: #4CAF50;
            }}
            .regression {{
                color: #F44336;
            }}
            .chart-container {{
                margin: 30px 0;
                border: 1px solid #eee;
                padding: 20px;
                border-radius: 5px;
            }}
            footer {{
                text-align: center;
                margin-top: 40px;
                padding-top: 20px;
                border-top: 1px solid #eee;
                color: #777;
                font-size: 0.9em;
            }}
        </style>
    </head>
    <body>
        <h1>éŸ³é¢‘å‚æ•°æ¨¡å‹è¯„ä¼°æŠ¥å‘Š</h1>
        
        <div class="report-meta">
            <p><strong>ç”Ÿæˆæ—¶é—´ï¼š</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            <p><strong>è¯„ä¼°é…ç½®ï¼š</strong></p>
            <ul>
                <li>éšæœºç§å­: {config.get('seed', 'N/A')}</li>
                <li>è¯„ä¼°æ ·æœ¬æ•°: {config.get('sample_count', 'N/A')}</li>
                <li>è¿è¡Œè®¾å¤‡: {config.get('device', 'N/A')}</li>
            </ul>
        </div>
        
        <h2>è¯„ä¼°æ‘˜è¦</h2>
        <table class="metric-table">
            <tr>
                <th>æŒ‡æ ‡</th>
                <th>åŸºç¡€æ¨¡å‹</th>
                <th>å¾®è°ƒæ¨¡å‹</th>
                <th>æå‡</th>
            </tr>
    """
    
    # æ·»åŠ æ ·æœ¬çº§æŒ‡æ ‡
    for metric in ['accuracy', 'exact_match']:
        base_value = base_metrics['sample_metrics'][metric]
        ft_value = ft_metrics['sample_metrics'][metric]
        change = ft_value - base_value
        change_pct = (change / base_value * 100) if base_value > 0 else float('inf')
        
        css_class = "improvement" if change > 0 else "regression"
        report += f"""
            <tr>
                <td>{metric.replace('_', ' ').title()}</td>
                <td>{base_value:.4f}</td>
                <td>{ft_value:.4f}</td>
                <td class="{css_class}">{change_pct:+.2f}%</td>
            </tr>
        """
    
    # æ·»åŠ å¾®è§‚æŒ‡æ ‡
    for metric in ['precision', 'recall', 'f1']:
        base_value = base_metrics['micro_metrics'][metric]
        ft_value = ft_metrics['micro_metrics'][metric]
        change = ft_value - base_value
        change_pct = (change / base_value * 100) if base_value > 0 else float('inf')
        
        css_class = "improvement" if change > 0 else "regression"
        report += f"""
            <tr>
                <td>Micro-{metric.title()}</td>
                <td>{base_value:.4f}</td>
                <td>{ft_value:.4f}</td>
                <td class="{css_class}">{change_pct:+.2f}%</td>
            </tr>
        """
    
    # æ ·æœ¬æ•°é‡
    report += f"""
            <tr>
                <td>æ ·æœ¬æ•°é‡</td>
                <td>{base_metrics['sample_count']}</td>
                <td>{ft_metrics['sample_count']}</td>
                <td>-</td>
            </tr>
        </table>
        
        <h2>å„æ ‡ç­¾F1åˆ†æ•°</h2>
        <table>
            <tr>
                <th>æ ‡ç­¾</th>
                <th>åŸºç¡€æ¨¡å‹</th>
                <th>å¾®è°ƒæ¨¡å‹</th>
                <th>æå‡</th>
            </tr>
    """
    
    # æ·»åŠ æ ‡ç­¾çº§æŒ‡æ ‡
    for label in sorted(VALID_LABELS):
        base_f1 = base_metrics['label_metrics'][label]['f1']
        ft_f1 = ft_metrics['label_metrics'][label]['f1']
        change = ft_f1 - base_f1
        change_pct = (change / base_f1 * 100) if base_f1 > 0 else float('inf')
        
        css_class = "improvement" if change > 0 else "regression"
        report += f"""
            <tr>
                <td>{label}</td>
                <td>{base_f1:.4f}</td>
                <td>{ft_f1:.4f}</td>
                <td class="{css_class}">{change_pct:+.2f}%</td>
            </tr>
        """
    
    report += "</table>"
    
    # æ·»åŠ é¢„æµ‹ç¤ºä¾‹
    report += """
        <h2>é¢„æµ‹ç¤ºä¾‹</h2>
    """
    
    for i, example in enumerate(examples[:5]):
        input_text = example['input']
        true_labels = example['true_labels']
        base_pred = example['base_prediction']
        ft_pred = example['ft_prediction']
        
        base_correct = set(true_labels) == set(base_pred)
        ft_correct = set(true_labels) == set(ft_pred)
        
        base_class = "correct" if base_correct else "incorrect"
        ft_class = "correct" if ft_correct else "incorrect"
        
        report += f"""
        <div class="example-container">
            <h3>ç¤ºä¾‹ {i+1}</h3>
            <p><strong>è¾“å…¥:</strong> {input_text}</p>
            <p><strong>çœŸå®æ ‡ç­¾:</strong> {', '.join(true_labels)}</p>
            <p><strong>åŸºç¡€æ¨¡å‹é¢„æµ‹:</strong> <span class="{base_class}">{', '.join(base_pred)}</span></p>
            <p><strong>å¾®è°ƒæ¨¡å‹é¢„æµ‹:</strong> <span class="{ft_class}">{', '.join(ft_pred)}</span></p>
        </div>
        """
    
    # æ·»åŠ é¡µè„š
    report += """
        <footer>
            <p>æœ¬æŠ¥å‘Šç”±éŸ³é¢‘å‚æ•°æ¨¡å‹è¯„ä¼°å·¥å…·è‡ªåŠ¨ç”Ÿæˆ</p>
        </footer>
    </body>
    </html>
    """
    
    return report

# æ•°æ®å¤„ç†åŠŸèƒ½
def preprocess_test_data(data, sample_ratio=100, seed=42):
    """é¢„å¤„ç†æµ‹è¯•æ•°æ®"""
    # è®¾ç½®éšæœºç§å­
    random.seed(seed)
    
    # éªŒè¯æ•°æ®æ ¼å¼
    valid_data = []
    for idx, item in enumerate(data):
        if not all(k in item for k in ['instruction', 'input', 'output']):
            continue
            
        if not item['output'].strip():
            continue
            
        valid_data.append(item)
    
    # è®¡ç®—é‡‡æ ·æ•°é‡
    sample_count = max(1, int(len(valid_data) * sample_ratio / 100))
    
    # éšæœºé‡‡æ ·
    if sample_count < len(valid_data):
        valid_data = random.sample(valid_data, sample_count)
    
    return valid_data

def create_evaluation_metrics(all_true_labels, all_pred_labels, label_set):
    """è®¡ç®—å¤šæ ‡ç­¾è¯„ä¼°æŒ‡æ ‡"""
    # æ„å»ºäºŒå€¼åŒ–æ•°ç»„
    y_true = []
    y_pred = []
    
    for true_labels, pred_labels in zip(all_true_labels, all_pred_labels):
        true_vec = [1 if label in true_labels else 0 for label in label_set]
        pred_vec = [1 if label in pred_labels else 0 for label in label_set]
        y_true.append(true_vec)
        y_pred.append(pred_vec)
    
    # è®¡ç®—æ ‡ç­¾çº§æŒ‡æ ‡
    label_metrics = {}
    for i, label in enumerate(label_set):
        true_label = [row[i] for row in y_true]
        pred_label = [row[i] for row in y_pred]
        
        # é¿å…é™¤é›¶é”™è¯¯
        if sum(true_label) == 0 and sum(pred_label) == 0:
            precision = 1.0
        elif sum(true_label) == 0:
            precision = 0.0
        elif sum(pred_label) == 0:
            precision = 0.0
        else:
            precision = precision_score(true_label, pred_label, zero_division=0)
            
        recall = recall_score(true_label, pred_label, zero_division=0)
        f1 = f1_score(true_label, pred_label, zero_division=0)
        
        label_metrics[label] = {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'support': sum(true_label)
        }
    
    # æ ·æœ¬çº§æŒ‡æ ‡
    sample_accuracy = sum(1 for t, p in zip(all_true_labels, all_pred_labels) 
                          if len(set(t) & set(p)) > 0) / max(1, len(all_true_labels))
    
    exact_match = sum(1 for t, p in zip(all_true_labels, all_pred_labels) 
                     if set(t) == set(p)) / max(1, len(all_true_labels))
    
    # æ ‡ç­¾çº§æŒ‡æ ‡
    micro_precision = precision_score(y_true, y_pred, average='micro', zero_division=0)
    micro_recall = recall_score(y_true, y_pred, average='micro', zero_division=0)
    micro_f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)
    
    macro_precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
    macro_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)
    
    return {
        'sample_metrics': {
            'accuracy': sample_accuracy,
            'exact_match': exact_match
        },
        'micro_metrics': {
            'precision': micro_precision,
            'recall': micro_recall,
            'f1': micro_f1
        },
        'macro_metrics': {
            'precision': macro_precision,
            'recall': macro_recall,
            'f1': macro_f1
        },
        'label_metrics': label_metrics,
        'sample_count': len(all_true_labels)
    }

# ç•Œé¢åŠŸèƒ½
def show_welcome():
    """æ˜¾ç¤ºæ¬¢è¿é¡µé¢"""
    st.markdown('<h1 class="main-header">éŸ³é¢‘å‚æ•°æ¨¡å‹è¯„ä¼°å·¥å…·</h1>', unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown("""
        <div style="text-align: center; padding: 20px; background-color: #f5f9ff; border-radius: 10px; margin: 20px 0;">
            <img src="https://img.icons8.com/fluency/96/000000/musical-notes.png" style="width: 80px;">
            <h2 style="margin-top: 10px;">æ¬¢è¿ä½¿ç”¨æ¨¡å‹è¯„ä¼°å·¥å…·</h2>
            <p>è¿™ä¸ªå·¥å…·å¯ä»¥å¸®åŠ©æ‚¨æ¯”è¾ƒåŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹åœ¨éŸ³é¢‘å‚æ•°åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</p>
            <p>è¯·åœ¨å·¦ä¾§å¯¼èˆªæ ä¸­é€‰æ‹©è¦ä½¿ç”¨çš„åŠŸèƒ½ã€‚</p>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("""
    ### ä¸»è¦åŠŸèƒ½
    
    - **æ¨¡å‹è¯„ä¼°**ï¼šä¸Šä¼ æµ‹è¯•æ•°æ®é›†ï¼Œè¯„ä¼°ä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½
    - **æ•°æ®é›†åˆ†æ**ï¼šæŸ¥çœ‹æ•°æ®é›†çš„åˆ†å¸ƒå’Œç»Ÿè®¡ä¿¡æ¯
    - **ç»“æœå¯è§†åŒ–**ï¼šé€šè¿‡å›¾è¡¨ç›´è§‚æ¯”è¾ƒæ¨¡å‹æ€§èƒ½
    - **ç”ŸæˆæŠ¥å‘Š**ï¼šå¯¼å‡ºè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
    
    ### ä½¿ç”¨æ­¥éª¤
    
    1. åœ¨ä¾§è¾¹æ ä¸­é€‰æ‹©"å¼€å§‹è¯„ä¼°"
    2. ä¸Šä¼ æµ‹è¯•æ•°æ®é›†ï¼ˆJSONæ ¼å¼ï¼‰
    3. é…ç½®è¯„ä¼°å‚æ•°
    4. ç‚¹å‡»"å¼€å§‹è¯„ä¼°"æŒ‰é’®
    5. æŸ¥çœ‹è¯„ä¼°ç»“æœå’Œå¯è§†åŒ–
    6. å¯¼å‡ºè¯„ä¼°æŠ¥å‘Š
    """)

def run_evaluation(test_data, prompts, max_samples=50, seed=42):
    """è¿è¡Œæ¨¡å‹è¯„ä¼°"""
    # ç¤ºä¾‹è¯„ä¼°ï¼ˆåœ¨å®é™…åœºæ™¯ä¸­ä¼šè°ƒç”¨é¢„è®­ç»ƒæ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹ï¼‰
    # è¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®å±•ç¤ºç•Œé¢
    
    # è®¾ç½®è¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # æ¨¡æ‹ŸåŠ è½½æ¨¡å‹
    status_text.text("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    time.sleep(1)
    progress_bar.progress(10)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    status_text.text("å‡†å¤‡æµ‹è¯•æ•°æ®...")
    set_seed(seed)
    test_samples = preprocess_test_data(test_data, sample_ratio=100, seed=seed)
    test_samples = test_samples[:max_samples]  # é™åˆ¶æ ·æœ¬æ•°
    time.sleep(1)
    progress_bar.progress(20)
    
    # æ¨¡æ‹Ÿè¯„ä¼°è¿‡ç¨‹
    all_true_labels = []
    base_pred_labels = []
    ft_pred_labels = []
    examples = []
    
    total_samples = len(test_samples)
    
    for i, sample in enumerate(test_samples):
        # æ›´æ–°è¿›åº¦
        progress = 20 + int(70 * (i+1) / total_samples)
        progress_bar.progress(progress)
        status_text.text(f"è¯„ä¼°æ ·æœ¬ {i+1}/{total_samples}...")
        
        # æå–çœŸå®æ ‡ç­¾
        true_labels = parse_labels(sample['output'])
        
        # åœ¨å®é™…åœºæ™¯ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨æ¨¡å‹ç”Ÿæˆé¢„æµ‹
        # è¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        
        # æ¨¡æ‹ŸåŸºç¡€æ¨¡å‹é¢„æµ‹ï¼ˆéšæœºæ€§è¾ƒé«˜ï¼‰
        base_correct = random.random() < 0.4  # 40%æ­£ç¡®ç‡
        if base_correct:
            base_pred = true_labels
        else:
            # éšæœºé€‰æ‹©1-3ä¸ªæ ‡ç­¾
            available_labels = list(VALID_LABELS - set(true_labels))
            if available_labels:
                wrong_count = min(random.randint(1, 3), len(available_labels))
                wrong_labels = random.sample(available_labels, wrong_count)
                base_pred = wrong_labels
            else:
                base_pred = []
        
        # æ¨¡æ‹Ÿå¾®è°ƒæ¨¡å‹é¢„æµ‹ï¼ˆæ›´é«˜çš„æ­£ç¡®ç‡ï¼‰
        ft_correct = random.random() < 0.75  # 75%æ­£ç¡®ç‡
        if ft_correct:
            ft_pred = true_labels
        else:
            # éšæœºé€‰æ‹©1-2ä¸ªæ ‡ç­¾ï¼Œå¹¶æœ‰50%æ¦‚ç‡ä¿ç•™éƒ¨åˆ†æ­£ç¡®æ ‡ç­¾
            available_labels = list(VALID_LABELS - set(true_labels))
            if available_labels:
                wrong_count = min(random.randint(1, 2), len(available_labels))
                wrong_labels = random.sample(available_labels, wrong_count)
                
                # æœ‰50%å‡ ç‡ä¿ç•™éƒ¨åˆ†æ­£ç¡®æ ‡ç­¾
                if true_labels and random.random() < 0.5:
                    keep_count = random.randint(1, len(true_labels))
                    kept_true = random.sample(true_labels, keep_count)
                    ft_pred = wrong_labels + kept_true
                else:
                    ft_pred = wrong_labels
            else:
                ft_pred = []
        
        # æ”¶é›†ç»“æœ
        all_true_labels.append(true_labels)
        base_pred_labels.append(base_pred)
        ft_pred_labels.append(ft_pred)
        
        # æ”¶é›†ç¤ºä¾‹
        examples.append({
            'input': sample['input'],
            'true_labels': true_labels,
            'base_prediction': base_pred,
            'ft_prediction': ft_pred
        })
        
        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        time.sleep(0.01)
    
    # è®¡ç®—æŒ‡æ ‡
    status_text.text("è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    progress_bar.progress(95)
    
    base_metrics = create_evaluation_metrics(all_true_labels, base_pred_labels, list(VALID_LABELS))
    ft_metrics = create_evaluation_metrics(all_true_labels, ft_pred_labels, list(VALID_LABELS))
    
    # å®Œæˆè¯„ä¼°
    progress_bar.progress(100)
    status_text.text("è¯„ä¼°å®Œæˆï¼")
    time.sleep(0.5)
    status_text.empty()
    
    return {
        "base_model": base_metrics,
        "finetuned_model": ft_metrics
    }, examples

def display_evaluation_results(metrics, examples):
    """æ˜¾ç¤ºè¯„ä¼°ç»“æœ"""
    base_metrics = metrics["base_model"]
    ft_metrics = metrics["finetuned_model"]
    
    st.markdown('<h2 class="sub-header">è¯„ä¼°ç»“æœæ‘˜è¦</h2>', unsafe_allow_html=True)
    
    # ä¸»è¦æŒ‡æ ‡å¡ç‰‡
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown('<div class="metric-card">', unsafe_allow_html=True)
        st.markdown('<div class="metric-title">å‡†ç¡®ç‡</div>', unsafe_allow_html=True)
        
        base_acc = base_metrics['sample_metrics']['accuracy']
        ft_acc = ft_metrics['sample_metrics']['accuracy']
        change = ft_acc - base_acc
        change_pct = (change / base_acc * 100) if base_acc > 0 else float('inf')
        
        st.markdown(f'<div class="metric-value">{ft_acc:.2%}</div>', unsafe_allow_html=True)
        
        comparison_class = "metric-comparison" if change >= 0 else "metric-comparison negative-comparison"
        st.markdown(f'<div class="{comparison_class}">ä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”: {change_pct:+.1f}%</div>', unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)
    
    with col2:
        st.markdown('<div class="metric-card">', unsafe_allow_html=True)
        st.markdown('<div class="metric-title">F1åˆ†æ•° (å¾®å¹³å‡)</div>', unsafe_allow_html=True)
        
        base_f1 = base_metrics['micro_metrics']['f1']
        ft_f1 = ft_metrics['micro_metrics']['f1']
        change = ft_f1 - base_f1
        change_pct = (change / base_f1 * 100) if base_f1 > 0 else float('inf')
        
        st.markdown(f'<div class="metric-value">{ft_f1:.2%}</div>', unsafe_allow_html=True)
        
        comparison_class = "metric-comparison" if change >= 0 else "metric-comparison negative-comparison"
        st.markdown(f'<div class="{comparison_class}">ä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”: {change_pct:+.1f}%</div>', unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)
    
    with col3:
        st.markdown('<div class="metric-card">', unsafe_allow_html=True)
        st.markdown('<div class="metric-title">å®Œå…¨åŒ¹é…ç‡</div>', unsafe_allow_html=True)
        
        base_exact = base_metrics['sample_metrics']['exact_match']
        ft_exact = ft_metrics['sample_metrics']['exact_match']
        change = ft_exact - base_exact
        change_pct = (change / base_exact * 100) if base_exact > 0 else float('inf')
        
        st.markdown(f'<div class="metric-value">{ft_exact:.2%}</div>', unsafe_allow_html=True)
        
        comparison_class = "metric-comparison" if change >= 0 else "metric-comparison negative-comparison"
        st.markdown(f'<div class="{comparison_class}">ä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”: {change_pct:+.1f}%</div>', unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)
    
    # è¯¦ç»†æŒ‡æ ‡è¡¨æ ¼
    st.markdown('<h3 class="sub-header">è¯¦ç»†è¯„ä¼°æŒ‡æ ‡</h3>', unsafe_allow_html=True)
    
# åˆ›å»ºDataFrame
    metrics_df = pd.DataFrame({
        'æŒ‡æ ‡': ['å‡†ç¡®ç‡', 'å®Œå…¨åŒ¹é…ç‡', 'å¾®å¹³å‡ç²¾ç¡®ç‡', 'å¾®å¹³å‡å¬å›ç‡', 'å¾®å¹³å‡F1', 'å®å¹³å‡ç²¾ç¡®ç‡', 'å®å¹³å‡å¬å›ç‡', 'å®å¹³å‡F1'],
        'åŸºç¡€æ¨¡å‹': [
            base_metrics['sample_metrics']['accuracy'],
            base_metrics['sample_metrics']['exact_match'],
            base_metrics['micro_metrics']['precision'],
            base_metrics['micro_metrics']['recall'],
            base_metrics['micro_metrics']['f1'],
            base_metrics['macro_metrics']['precision'],
            base_metrics['macro_metrics']['recall'],
            base_metrics['macro_metrics']['f1']
        ],
        'å¾®è°ƒæ¨¡å‹': [
            ft_metrics['sample_metrics']['accuracy'],
            ft_metrics['sample_metrics']['exact_match'],
            ft_metrics['micro_metrics']['precision'],
            ft_metrics['micro_metrics']['recall'],
            ft_metrics['micro_metrics']['f1'],
            ft_metrics['macro_metrics']['precision'],
            ft_metrics['macro_metrics']['recall'],
            ft_metrics['macro_metrics']['f1']
        ]
    })
    
    # è®¡ç®—æå‡
    metrics_df['æå‡'] = metrics_df['å¾®è°ƒæ¨¡å‹'] - metrics_df['åŸºç¡€æ¨¡å‹']
    metrics_df['æå‡ç™¾åˆ†æ¯”'] = metrics_df.apply(
        lambda row: ((row['å¾®è°ƒæ¨¡å‹'] - row['åŸºç¡€æ¨¡å‹']) / row['åŸºç¡€æ¨¡å‹'] * 100) if row['åŸºç¡€æ¨¡å‹'] > 0 else float('inf'),
        axis=1
    )
    
    # æ ¼å¼åŒ–ä¸ºç™¾åˆ†æ¯”
    format_cols = ['åŸºç¡€æ¨¡å‹', 'å¾®è°ƒæ¨¡å‹', 'æå‡']
    metrics_df[format_cols] = metrics_df[format_cols].applymap(lambda x: f"{x:.2%}")
    metrics_df['æå‡ç™¾åˆ†æ¯”'] = metrics_df['æå‡ç™¾åˆ†æ¯”'].apply(lambda x: f"{x:+.2f}%" if x != float('inf') else "N/A")
    
    st.dataframe(metrics_df)
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨
    st.markdown('<h3 class="sub-header">æ€§èƒ½å¯¹æ¯”</h3>', unsafe_allow_html=True)
    
    fig = go.Figure()
    metrics_to_plot = ['å‡†ç¡®ç‡', 'å®Œå…¨åŒ¹é…ç‡', 'å¾®å¹³å‡F1', 'å®å¹³å‡F1']
    
    base_values = [float(row['åŸºç¡€æ¨¡å‹'].strip('%'))/100 for _, row in metrics_df[metrics_df['æŒ‡æ ‡'].isin(metrics_to_plot)].iterrows()]
    ft_values = [float(row['å¾®è°ƒæ¨¡å‹'].strip('%'))/100 for _, row in metrics_df[metrics_df['æŒ‡æ ‡'].isin(metrics_to_plot)].iterrows()]
    
    fig.add_trace(go.Bar(
        x=metrics_to_plot,
        y=base_values,
        name='åŸºç¡€æ¨¡å‹',
        marker_color='#90CAF9'
    ))
    
    fig.add_trace(go.Bar(
        x=metrics_to_plot,
        y=ft_values,
        name='å¾®è°ƒæ¨¡å‹',
        marker_color='#F48FB1'
    ))
    
    fig.update_layout(
        barmode='group',
        title='æ¨¡å‹æ€§èƒ½å¯¹æ¯”',
        yaxis=dict(
            title='å¾—åˆ†',
            tickformat='.0%'
        ),
        xaxis=dict(
            title='è¯„ä¼°æŒ‡æ ‡'
        ),
        legend=dict(
            x=0,
            y=1.1,
            orientation='h'
        ),
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # æ ‡ç­¾çº§æ€§èƒ½
    st.markdown('<h3 class="sub-header">å„æ ‡ç­¾æ€§èƒ½</h3>', unsafe_allow_html=True)
    
    label_df = pd.DataFrame({
        'æ ‡ç­¾': list(VALID_LABELS),
        'åŸºç¡€æ¨¡å‹ F1': [base_metrics['label_metrics'][label]['f1'] for label in VALID_LABELS],
        'å¾®è°ƒæ¨¡å‹ F1': [ft_metrics['label_metrics'][label]['f1'] for label in VALID_LABELS],
        'æ ·æœ¬æ•°é‡': [base_metrics['label_metrics'][label]['support'] for label in VALID_LABELS]
    })
    
    label_df['æå‡'] = label_df['å¾®è°ƒæ¨¡å‹ F1'] - label_df['åŸºç¡€æ¨¡å‹ F1']
    label_df['æå‡ç™¾åˆ†æ¯”'] = label_df.apply(
        lambda row: ((row['å¾®è°ƒæ¨¡å‹ F1'] - row['åŸºç¡€æ¨¡å‹ F1']) / row['åŸºç¡€æ¨¡å‹ F1'] * 100) if row['åŸºç¡€æ¨¡å‹ F1'] > 0 else float('inf'),
        axis=1
    )
    
    # æ ¼å¼åŒ–ä¸ºç™¾åˆ†æ¯”
    format_cols = ['åŸºç¡€æ¨¡å‹ F1', 'å¾®è°ƒæ¨¡å‹ F1']
    label_df[format_cols] = label_df[format_cols].applymap(lambda x: f"{x:.2%}")
    label_df['æå‡'] = label_df['æå‡'].apply(lambda x: f"{x:.2%}")
    label_df['æå‡ç™¾åˆ†æ¯”'] = label_df['æå‡ç™¾åˆ†æ¯”'].apply(lambda x: f"{x:+.2f}%" if x != float('inf') else "N/A")
    
    st.dataframe(label_df)
    
    # ç»˜åˆ¶æ ‡ç­¾çº§æ€§èƒ½å¯¹æ¯”å›¾
    fig = px.bar(
        label_df,
        x='æ ‡ç­¾',
        y=['åŸºç¡€æ¨¡å‹ F1', 'å¾®è°ƒæ¨¡å‹ F1'],
        barmode='group',
        labels={'value': 'F1åˆ†æ•°', 'variable': 'æ¨¡å‹'},
        title='å„æ ‡ç­¾F1åˆ†æ•°å¯¹æ¯”',
        color_discrete_map={
            'åŸºç¡€æ¨¡å‹ F1': '#90CAF9',
            'å¾®è°ƒæ¨¡å‹ F1': '#F48FB1'
        }
    )
    
    fig.update_layout(
        xaxis=dict(title='æ ‡ç­¾'),
        yaxis=dict(title='F1åˆ†æ•°', tickformat='.0%'),
        legend=dict(
            x=0,
            y=1.1,
            orientation='h'
        ),
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # é¢„æµ‹ç¤ºä¾‹
    st.markdown('<h3 class="sub-header">é¢„æµ‹ç¤ºä¾‹</h3>', unsafe_allow_html=True)
    
    for i, example in enumerate(examples[:5]):
        with st.expander(f"ç¤ºä¾‹ {i+1}: {example['input'][:50]}...", expanded=(i==0)):
            st.markdown(f"**è¾“å…¥æ–‡æœ¬**: {example['input']}")
            st.markdown(f"**çœŸå®æ ‡ç­¾**: {', '.join(example['true_labels'])}")
            
            base_correct = set(example['true_labels']) == set(example['base_prediction'])
            ft_correct = set(example['true_labels']) == set(example['ft_prediction'])
            
            base_class = "correct-prediction" if base_correct else "incorrect-prediction"
            ft_class = "correct-prediction" if ft_correct else "incorrect-prediction"
            
            st.markdown(f"**åŸºç¡€æ¨¡å‹é¢„æµ‹**: <span class='{base_class}'>{', '.join(example['base_prediction'])}</span>", unsafe_allow_html=True)
            st.markdown(f"**å¾®è°ƒæ¨¡å‹é¢„æµ‹**: <span class='{ft_class}'>{', '.join(example['ft_prediction'])}</span>", unsafe_allow_html=True)

def display_data_analysis(test_data):
    """æ˜¾ç¤ºæ•°æ®é›†åˆ†æ"""
    st.markdown('<h2 class="sub-header">æ•°æ®é›†åˆ†æ</h2>', unsafe_allow_html=True)
    
    # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    st.markdown("### åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯")
    
    total_samples = len(test_data)
    
    # æå–å¹¶è®¡ç®—æ ‡ç­¾åˆ†å¸ƒ
    all_labels = []
    for item in test_data:
        labels = parse_labels(item.get('output', ''))
        all_labels.extend(labels)
    
    # æ ‡ç­¾åˆ†å¸ƒ
    label_counts = {}
    for label in VALID_LABELS:
        label_counts[label] = all_labels.count(label)
    
    # å¤šæ ‡ç­¾åˆ†å¸ƒ
    multi_label_counts = {}
    for item in test_data:
        labels = parse_labels(item.get('output', ''))
        count = len(labels)
        multi_label_counts[count] = multi_label_counts.get(count, 0) + 1
    
    # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("æ ·æœ¬æ€»æ•°", total_samples)
    
    with col2:
        avg_labels = len(all_labels) / max(1, total_samples)
        st.metric("å¹³å‡æ ‡ç­¾æ•°", f"{avg_labels:.2f}")
    
    with col3:
        unique_labels = len(set(all_labels))
        st.metric("ä½¿ç”¨çš„ä¸åŒæ ‡ç­¾æ•°", f"{unique_labels}/{len(VALID_LABELS)}")
    
    # æ ‡ç­¾åˆ†å¸ƒå›¾
    st.markdown("### æ ‡ç­¾åˆ†å¸ƒ")
    
    label_df = pd.DataFrame({
        'æ ‡ç­¾': list(label_counts.keys()),
        'å‡ºç°æ¬¡æ•°': list(label_counts.values())
    })
    
    label_df = label_df.sort_values('å‡ºç°æ¬¡æ•°', ascending=False)
    
    fig = px.bar(
        label_df,
        x='æ ‡ç­¾',
        y='å‡ºç°æ¬¡æ•°',
        title='æ ‡ç­¾åˆ†å¸ƒ',
        color='å‡ºç°æ¬¡æ•°',
        color_continuous_scale='Blues'
    )
    
    fig.update_layout(
        xaxis=dict(title='æ ‡ç­¾'),
        yaxis=dict(title='å‡ºç°æ¬¡æ•°'),
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # å¤šæ ‡ç­¾åˆ†å¸ƒå›¾
    st.markdown("### å¤šæ ‡ç­¾åˆ†å¸ƒ")
    
    multi_label_df = pd.DataFrame({
        'æ ‡ç­¾æ•°é‡': list(multi_label_counts.keys()),
        'æ ·æœ¬æ•°': list(multi_label_counts.values())
    })
    
    multi_label_df = multi_label_df.sort_values('æ ‡ç­¾æ•°é‡')
    
    fig = px.bar(
        multi_label_df,
        x='æ ‡ç­¾æ•°é‡',
        y='æ ·æœ¬æ•°',
        title='å¤šæ ‡ç­¾åˆ†å¸ƒ',
        color='æ ·æœ¬æ•°',
        color_continuous_scale='Reds'
    )
    
    fig.update_layout(
        xaxis=dict(title='æ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾æ•°é‡'),
        yaxis=dict(title='æ ·æœ¬æ•°'),
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # æ ‡ç­¾å…±ç°çŸ©é˜µ
    st.markdown("### æ ‡ç­¾å…±ç°çŸ©é˜µ")
    
    # åˆ›å»ºå…±ç°çŸ©é˜µ
    cooccurrence = np.zeros((len(VALID_LABELS), len(VALID_LABELS)))
    valid_labels_list = list(VALID_LABELS)
    
    for item in test_data:
        labels = parse_labels(item.get('output', ''))
        for i, label1 in enumerate(valid_labels_list):
            for j, label2 in enumerate(valid_labels_list):
                if label1 in labels and label2 in labels:
                    cooccurrence[i, j] += 1
    
    # ç»˜åˆ¶çƒ­å›¾
    fig = px.imshow(
        cooccurrence,
        x=valid_labels_list,
        y=valid_labels_list,
        color_continuous_scale='Viridis',
        title='æ ‡ç­¾å…±ç°çŸ©é˜µ'
    )
    
    fig.update_layout(
        height=600,
        width=600
    )
    
    st.plotly_chart(fig)
    
    # æ˜¾ç¤ºæ•°æ®æ ·æœ¬
    st.markdown("### æ•°æ®æ ·æœ¬é¢„è§ˆ")
    
    samples = test_data[:10]  # é™åˆ¶æ˜¾ç¤ºå‰10ä¸ªæ ·æœ¬
    for i, sample in enumerate(samples):
        with st.expander(f"æ ·æœ¬ {i+1}", expanded=(i==0)):
            st.markdown(f"**æŒ‡ä»¤**: {sample.get('instruction', 'N/A')}")
            st.markdown(f"**è¾“å…¥**: {sample.get('input', 'N/A')}")
            st.markdown(f"**è¾“å‡º**: {sample.get('output', 'N/A')}")
            
            # è§£ææ ‡ç­¾
            labels = parse_labels(sample.get('output', ''))
            st.markdown(f"**è§£æåçš„æ ‡ç­¾**: {', '.join(labels)}")

def main():
    """ä¸»å‡½æ•°"""
    # ä¾§è¾¹æ å¯¼èˆª
    st.sidebar.markdown("## å¯¼èˆª")
    page = st.sidebar.radio(
        "é€‰æ‹©åŠŸèƒ½",
        ["æ¬¢è¿é¡µé¢", "å¼€å§‹è¯„ä¼°", "æ•°æ®é›†åˆ†æ"]
    )
    
    # ä¾§è¾¹æ é…ç½®
    st.sidebar.markdown("## é…ç½®")
    
    # ä¸Šä¼ æµ‹è¯•æ•°æ®
    uploaded_file = st.sidebar.file_uploader("ä¸Šä¼ æµ‹è¯•æ•°æ®é›†", type=["json"])
    
    # ç¤ºä¾‹æ•°æ®
    if uploaded_file is None:
        st.sidebar.info("è¯·ä¸Šä¼ JSONæ ¼å¼çš„æµ‹è¯•æ•°æ®é›†ã€‚å¦‚æœæ²¡æœ‰æ•°æ®é›†ï¼Œå°†ä½¿ç”¨ç¤ºä¾‹æ•°æ®ã€‚")
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        test_data = []
        for i in range(100):
            # éšæœºé€‰æ‹©1-3ä¸ªæ ‡ç­¾
            label_count = random.randint(1, 3)
            labels = random.sample(list(VALID_LABELS), label_count)
            
            # åˆ›å»ºæ ·æœ¬
            sample = {
                "instruction": "åˆ†æéŸ³é¢‘å¤„ç†éœ€æ±‚å¹¶é€‰æ‹©ç›¸å…³å‚æ•°ï¼ˆ1-3ä¸ªï¼‰ï¼šé€‰é¡¹ï¼šä½é¢‘/ä¸­é¢‘/é«˜é¢‘/reverb/æ•ˆæœå™¨/å£°åœº/å‹ç¼©/éŸ³é‡",
                "input": f"ç¤ºä¾‹è¾“å…¥æ–‡æœ¬ {i+1}",
                "output": ", ".join(labels)
            }
            test_data.append(sample)
    else:
        try:
            test_data = json.load(uploaded_file)
            st.sidebar.success(f"æˆåŠŸåŠ è½½æ•°æ®é›†ï¼ŒåŒ…å« {len(test_data)} ä¸ªæ ·æœ¬ã€‚")
        except Exception as e:
            st.sidebar.error(f"åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")
            test_data = []
    
    # å…¶ä»–é…ç½®
    max_samples = st.sidebar.slider("è¯„ä¼°æ ·æœ¬æ•°é‡", 10, 200, DEFAULT_MAX_SAMPLES)
    sample_ratio = st.sidebar.slider("æ ·æœ¬æ¯”ä¾‹ (%)", 1, 100, 100)
    seed = st.sidebar.number_input("éšæœºç§å­", value=DEFAULT_SEED)
    
    # æ¨¡å‹æç¤ºè¯é…ç½®
    with st.sidebar.expander("æç¤ºè¯é…ç½®", expanded=False):
        base_prompt = st.text_area(
            "åŸºç¡€æ¨¡å‹æç¤ºè¯",
            value="è¾“å…¥ï¼š[TEXT]\nè¾“å‡ºï¼š"
        )
        
        ft_prompt = st.text_area(
            "å¾®è°ƒæ¨¡å‹æç¤ºè¯",
            value="åˆ†æéŸ³é¢‘å¤„ç†éœ€æ±‚å¹¶é€‰æ‹©ç›¸å…³å‚æ•°ï¼ˆ1-3ä¸ªï¼‰ï¼š\n\nè¾“å…¥ï¼š[TEXT]\nè¾“å‡ºï¼š"
        )
    
    # æ¨¡å‹è·¯å¾„é…ç½®
    with st.sidebar.expander("æ¨¡å‹è·¯å¾„é…ç½®", expanded=False):
        base_model_path = st.text_input(
            "åŸºç¡€æ¨¡å‹è·¯å¾„",
            value="../../models/base_model"
        )
        
        ft_model_path = st.text_input(
            "å¾®è°ƒæ¨¡å‹è·¯å¾„",
            value="../../models/finetuned_model"
        )
    
    # åˆ›å»ºæç¤ºè¯å­—å…¸
    prompts = {
        "base": base_prompt,
        "finetuned": ft_prompt
    }
    
    # é¡µé¢å†…å®¹
    if page == "æ¬¢è¿é¡µé¢":
        show_welcome()
    
    elif page == "å¼€å§‹è¯„ä¼°":
        st.markdown('<h1 class="main-header">æ¨¡å‹è¯„ä¼°</h1>', unsafe_allow_html=True)
        
        if len(test_data) == 0:
            st.warning("è¯·ä¸Šä¼ æµ‹è¯•æ•°æ®é›†æˆ–ä½¿ç”¨ç¤ºä¾‹æ•°æ®ã€‚")
            st.stop()
        
        # è¯„ä¼°æŒ‰é’®
        if st.button("ğŸš€ å¼€å§‹è¯„ä¼°", key="run_evaluation"):
            with st.spinner("æ­£åœ¨è¯„ä¼°ä¸­..."):
                metrics, examples = run_evaluation(test_data, prompts, max_samples, seed)
                
                # ä¿å­˜ç»“æœåˆ°ä¼šè¯çŠ¶æ€
                st.session_state.metrics = metrics
                st.session_state.examples = examples
                st.session_state.config = {
                    "seed": seed,
                    "sample_count": min(max_samples, len(test_data)),
                    "sample_ratio": sample_ratio,
                    "device": "CPU" if not torch.cuda.is_available() else "GPU"
                }
        
        # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
        if 'metrics' in st.session_state and 'examples' in st.session_state:
            display_evaluation_results(st.session_state.metrics, st.session_state.examples)
            
            # å¯¼å‡ºé€‰é¡¹
            st.markdown("### å¯¼å‡ºç»“æœ")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # ç”ŸæˆJSONä¸‹è½½é“¾æ¥
                export_data = {
                    "metrics": st.session_state.metrics,
                    "config": st.session_state.config
                }
                st.markdown(
                    get_download_link(export_data, "evaluation_results.json", "ğŸ“Š ä¸‹è½½è¯„ä¼°ç»“æœ (JSON)"),
                    unsafe_allow_html=True
                )
            
            with col2:
                # ç”ŸæˆHTMLæŠ¥å‘Šä¸‹è½½é“¾æ¥
                report_html = generate_html_report(
                    st.session_state.metrics, 
                    st.session_state.examples,
                    st.session_state.config
                )
                
                b64 = base64.b64encode(report_html.encode()).decode()
                href = f'<a href="data:text/html;base64,{b64}" download="æ¨¡å‹è¯„ä¼°æŠ¥å‘Š.html">ğŸ“„ ä¸‹è½½å®Œæ•´è¯„ä¼°æŠ¥å‘Š (HTML)</a>'
                st.markdown(href, unsafe_allow_html=True)a
    
    elif page == "æ•°æ®é›†åˆ†æ":
        st.markdown('<h1 class="main-header">æ•°æ®é›†åˆ†æ</h1>', unsafe_allow_html=True)
        
        if len(test_data) == 0:
            st.warning("è¯·ä¸Šä¼ æµ‹è¯•æ•°æ®é›†æˆ–ä½¿ç”¨ç¤ºä¾‹æ•°æ®ã€‚")
            st.stop()
        
        display_data_analysis(test_data)
    
    # é¡µè„š
    st.markdown("""
    <footer>
        <p>éŸ³é¢‘å‚æ•°æ¨¡å‹è¯„ä¼°å·¥å…· | ç‰ˆæœ¬ 1.0 | åˆ¶ä½œè€…ï¼šMixMasterå›¢é˜Ÿ</p>
    </footer>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()