"""
æ··éŸ³éœ€æ±‚å¢å¼ºç³»ç»Ÿ v7.2
(ç»ˆæä¼˜åŒ–ç‰ˆ)
"""
import os
import json
import re
import random
import time
import jieba
import torch
import numpy as np
from datetime import datetime
from packaging import version
from docx import Document
from docx.shared import Pt
from transformers import AutoTokenizer, AutoModelForCausalLM
from concurrent.futures import ThreadPoolExecutor

# é…ç½®è·¯å¾„
CONFIG_PATH = "/Volumes/Study/prj/config/music_synonyms.json"
MODEL_PATH = "/Volumes/Study/prj/models/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa"
INPUT_FILE = "/Volumes/Study/prj/data/raw/training_raw_data.docx"
OUTPUT_FILE = "/Volumes/Study/prj/data/processed/augmented_results_final.docx"

# å…¨å±€é…ç½®
MAX_TEXT_LENGTH = 150
MIN_TEXT_LENGTH = 8
BATCH_SIZE = 16  # å¢å¤§æ‰¹é‡å¤„ç†
CACHE_SIZE = 100  # å†…å­˜ç¼“å­˜æ¡æ•°

class HybridAugmentorPro:
    def __init__(self):
        # ç¡¬ä»¶åˆå§‹åŒ–
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        self._configure_hardware()
        
        # åŠ è½½èµ„æº
        self.term_dict = self._load_terminology()
        self.tokenizer, self.model = self._load_model()
        self.executor = ThreadPoolExecutor(max_workers=2)
        jieba.initialize()
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def _configure_hardware(self):
        """ç¡¬ä»¶ä¼˜åŒ–é…ç½®"""
        if self.device.type == "mps":
            torch.mps.set_per_process_memory_fraction(0.9)
            torch.mps.empty_cache()
            print("âœ… MPSåŠ é€Ÿå·²å¯ç”¨")
            
    def _load_terminology(self):
        """åŠ è½½æœ¯è¯­åº“"""
        with open(CONFIG_PATH) as f:
            return {
                term: aliases if isinstance(aliases, list) else [aliases]
                for cat in json.load(f).values() for term, aliases in cat.items()
            }

    def _load_model(self):
        """ä¼˜åŒ–æ¨¡å‹åŠ è½½"""
        try:
            tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                device_map=self.device,
                torch_dtype=torch.float16,
                trust_remote_code=True
            ).eval()
            print(f"ğŸ›ï¸ æ¨¡å‹åŠ è½½æˆåŠŸ | è®¾å¤‡: {model.device}")
            return tokenizer, model
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")

    def _term_replace(self, text):
        """å¢å¼ºå‹æœ¯è¯­æ›¿æ¢"""
        return ''.join([
            random.choice(self.term_dict.get(word, [word]))
            if random.random() < 0.85 else word 
            for word in jieba.lcut(text)
        ])

    def _generate_batch(self, texts):
        """æ‰¹é‡ç”Ÿæˆä¼˜åŒ–"""
        prompts = [
            f"ç”Ÿæˆ3ä¸ªä¸“ä¸šæ··éŸ³å˜ä½“ï¼ˆä»…æœ¯è¯­ï¼‰ï¼š{text}"
            for text in texts
        ]
        
        inputs = self.tokenizer(
            prompts, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=128
        ).to(self.device)
        
        outputs = self.model.generate(
            inputs.input_ids,
            max_new_tokens=64,
            temperature=0.5,
            top_p=0.85,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id
        )
        
        return [
            re.findall(r"å˜ä½“[:ï¼š]?\s*(.+?)(?=\n|$)", 
            self.tokenizer.decode(o, skip_special_tokens=True))[:3] 
            for o in outputs
        ]

    def process_text(self, text):
        """å¤„ç†æµæ°´çº¿"""
        # æœ¯è¯­æ›¿æ¢
        eda_variant = self._term_replace(text)
        
        # æ¨¡å‹ç”Ÿæˆ
        model_variants = self._generate_batch([text])[0]
        
        # ç»“æœè¿‡æ»¤
        valid_variants = [
            v for v in [eda_variant] + model_variants
            if (8 <= len(v) <= 150) 
            and (v != text)
            and not re.search(r"[\[\]ã€ã€‘()ï¼ˆï¼‰]", v)
        ][:5]
        
        return valid_variants

def main():
    # åˆå§‹åŒ–ç³»ç»Ÿ
    augmentor = HybridAugmentorPro()
    doc = Document()
    doc.styles['Normal'].font.name = 'å¾®è½¯é›…é»‘'
    
    # æ‰¹é‡è¯»å–
    input_doc = Document(INPUT_FILE)
    paragraphs = [p.text.strip() for p in input_doc.paragraphs if p.text.strip()]
    
    # ç¼“å­˜å¤„ç†
    cache = []
    start_time = time.time()
    
    for idx, text in enumerate(paragraphs, 1):
        try:
            # å¼‚æ­¥å¤„ç†
            future = augmentor.executor.submit(augmentor.process_text, text)
            variants = future.result()
            
            # å†™å…¥ç¼“å­˜
            cache.append((text, variants))
            
            # æ‰¹é‡å†™å…¥
            if len(cache) >= CACHE_SIZE or idx == len(paragraphs):
                for orig, vars in cache:
                    p = doc.add_paragraph()
                    p.add_run(f"åŸå§‹éœ€æ±‚ï¼š{orig}").bold = True
                    doc.add_paragraph("ç”Ÿæˆå˜ä½“ï¼š" + " | ".join(vars))
                cache = []
                
            print(f"\rå¤„ç†è¿›åº¦: {idx}/{len(paragraphs)} | è€—æ—¶: {time.time()-start_time:.1f}s", end="")
            
        except Exception as e:
            print(f"\nâš ï¸ å¤„ç†å¤±è´¥: {text[:20]}... | é”™è¯¯: {str(e)}")

    # ä¿å­˜ç»“æœ
    doc.save(OUTPUT_FILE)
    print(f"\nâœ… å¤„ç†å®Œæˆï¼ä¿å­˜è‡³: {OUTPUT_FILE}")

if __name__ == "__main__":
    print("=== æ··éŸ³éœ€æ±‚å¢å¼ºç³»ç»Ÿ v7.2 ===")
    start = time.time()
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    finally:
        print(f"æ€»è€—æ—¶: {time.time()-start:.1f}ç§’")